# æ¨¡å‹æ€§èƒ½è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ

å®Œå…¨è‡ªåŠ¨åŒ–çš„LLMå…³ç³»æŠ½å–æ¨¡å‹è¯„ä¼°ç³»ç»Ÿï¼Œæ— éœ€äººå·¥æ ‡æ³¨ï¼Œé€šè¿‡8ä¸ªç»´åº¦å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ“‚ æ–‡ä»¶ç»“æ„

```
scripts/evaluation/
â”œâ”€â”€ model_evaluator.py           # æ ¸å¿ƒè¯„ä¼°å™¨(800+è¡Œ)
â”œâ”€â”€ run_evaluation.py            # ä¸€é”®è¿è¡Œè„šæœ¬
â”œâ”€â”€ test_evaluation.py           # ç¯å¢ƒè‡ªæ£€è„šæœ¬
â”œâ”€â”€ requirements_evaluation.txt  # ä¾èµ–åŒ…
â”œâ”€â”€ README.md                    # æœ¬æ–‡ä»¶
â””â”€â”€ EVALUATION_GUIDE.md         # å®Œæ•´ä½¿ç”¨æ–‡æ¡£
```

## ğŸš€ å¿«é€Ÿå¼€å§‹ (3æ­¥)

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
cd LLMRelationExtracter
pip install -r scripts/evaluation/requirements_evaluation.txt
```

### æ­¥éª¤2: è¿è¡Œè‡ªæ£€

```bash
python scripts/evaluation/test_evaluation.py
```

### æ­¥éª¤3: è¿è¡Œè¯„ä¼°

```bash
python scripts/evaluation/run_evaluation.py
```

## ğŸ“Š è¯„ä¼°ç»´åº¦

| ç»´åº¦ | æƒé‡ | è¯´æ˜ |
|------|------|------|
| **è´¨é‡è¯„åˆ†** | 30% | ç½®ä¿¡åº¦ã€ç¨³å®šæ€§ |
| **Schemaç¬¦åˆåº¦** | 20% | é…ç½®å†…æ¯”ä¾‹ã€è§„èŒƒæ€§ |
| **ä¸€è‡´æ€§** | 15% | å‘½åç»Ÿä¸€ã€åˆ†å¸ƒå‡åŒ€ |
| **å¤šæ ·æ€§** | 15% | ä¿¡æ¯è¦†ç›–å¹¿åº¦ |
| **Evidenceè´¨é‡** | 10% | è¯æ®å®Œæ•´æ€§ |
| **æˆæœ¬æ•ˆç›Š** | 10% | Tokenä½¿ç”¨æ•ˆç‡ |
| **é€Ÿåº¦** | - | å¤„ç†æ€§èƒ½ |
| **ç»¼åˆå¾—åˆ†** | - | 0-100åˆ† |

## ğŸ“ˆ è¾“å‡ºæŠ¥å‘Š

è¯„ä¼°ç»“æœä¿å­˜åœ¨ `data/output/evaluation_results/`:

- **ExcelæŠ¥å‘Š** - 5ä¸ªsheetè¯¦ç»†å¯¹æ¯”
- **MarkdownæŠ¥å‘Š** - æ’åå’Œåˆ†æ
- **å¯è§†åŒ–å›¾è¡¨** - é›·è¾¾å›¾ã€æŸ±çŠ¶å›¾ã€æ•£ç‚¹å›¾ã€åˆ†å¸ƒå›¾
- **JSONæ•°æ®** - å®Œæ•´è¯„ä¼°æ•°æ®

## ğŸ“– è¯¦ç»†æ–‡æ¡£

æŸ¥çœ‹ [EVALUATION_GUIDE.md](./EVALUATION_GUIDE.md) è·å–:
- è¯„ä¼°æŒ‡æ ‡è¯¦è§£
- æŠ¥å‘Šè§£è¯»æŒ‡å—
- è‡ªå®šä¹‰é…ç½®
- å¸¸è§é—®é¢˜FAQ

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ç”¨æ³•

```bash
# åœ¨LLMRelationExtracterç›®å½•ä¸‹è¿è¡Œ
cd LLMRelationExtracter
python scripts/evaluation/run_evaluation.py
```

### Python API

```python
from scripts.evaluation.model_evaluator import ModelEvaluator

evaluator = ModelEvaluator(output_dir="data/output/evaluation_results")
evaluator.load_model_output("model-name", "path/to/kg.json")
results = evaluator.evaluate_all_models()
evaluator.generate_comparison_report()
```

## ğŸ”§ é…ç½®æ¨¡å‹

ç¼–è¾‘ `scripts/evaluation/run_evaluation.py`:

```python
models_to_evaluate = {
    "deepseek-v3": "data/output/knowledge_graph_deepseek.json",
    "gemini-2.5-flash": "data/output/knowledge_graph_gemini-2.5-flash.json",
    "gpt-5": "data/output/knowledge_graph_gpt-5.json",
}
```

## âœ… ç‰¹ç‚¹

- âœ… **å®Œå…¨è‡ªåŠ¨åŒ–** - æ— éœ€äººå·¥æ ‡æ³¨
- âœ… **å¤šç»´åº¦è¯„ä¼°** - 8ä¸ªç»´åº¦å…¨é¢å¯¹æ¯”
- âœ… **å¯è§†åŒ–æŠ¥å‘Š** - å›¾è¡¨ç›´è§‚æ˜“æ‡‚
- âœ… **å¯å®šåˆ¶åŒ–** - æƒé‡ã€æŒ‡æ ‡å¯è°ƒæ•´
- âœ… **æ ‡å‡†åŒ–æµç¨‹** - å¯é‡å¤è¯„ä¼°

## ğŸ› å¸¸è§é—®é¢˜

### Q: ä¾èµ–åŒ…å®‰è£…å¤±è´¥ï¼Ÿ
```bash
pip install numpy pandas matplotlib seaborn openpyxl -i https://pypi.tuna.tsinghua.edu.cn/simple
```

### Q: æ‰¾ä¸åˆ°æ¨¡å‹è¾“å‡ºæ–‡ä»¶ï¼Ÿ
ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œï¼Œä¸”å·²ç»ç”¨ `main.py` ç”Ÿæˆäº†çŸ¥è¯†å›¾è°±ã€‚

### Q: å¦‚ä½•ä¿®æ”¹è¯„ä¼°æƒé‡ï¼Ÿ
ç¼–è¾‘ `model_evaluator.py` ä¸­çš„ `_compute_overall_score` å‡½æ•°ã€‚

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-10
