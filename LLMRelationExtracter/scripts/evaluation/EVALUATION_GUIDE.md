# æ¨¡å‹æ€§èƒ½è‡ªåŠ¨åŒ–è¯„ä¼°ç³»ç»Ÿ

## ğŸ“– ç®€ä»‹

è¿™æ˜¯ä¸€ä¸ª**å®Œå…¨è‡ªåŠ¨åŒ–**çš„LLMå…³ç³»æŠ½å–æ¨¡å‹è¯„ä¼°ç³»ç»Ÿ,æ— éœ€äººå·¥æ ‡æ³¨,é€šè¿‡å¤šç»´åº¦æŒ‡æ ‡å¯¹æ¯”ä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚

## ğŸ¯ è¯„ä¼°ç»´åº¦

### 1. **è´¨é‡è¯„åˆ†** (æƒé‡30%)
- å¹³å‡ç½®ä¿¡åº¦
- é«˜ç½®ä¿¡åº¦(>0.9)ä¸‰å…ƒç»„æ¯”ä¾‹
- ä½ç½®ä¿¡åº¦(<0.7)ä¸‰å…ƒç»„æ¯”ä¾‹
- ç½®ä¿¡åº¦ç¨³å®šæ€§(æ ‡å‡†å·®)

### 2. **Schemaç¬¦åˆåº¦** (æƒé‡20%)
- é…ç½®å†…ä¸‰å…ƒç»„æ¯”ä¾‹
- å…³ç³»ç±»å‹ç¬¦åˆåº¦
- å®ä½“ç±»å‹ç¬¦åˆåº¦

### 3. **ä¸€è‡´æ€§è¯„åˆ†** (æƒé‡15%)
- å®ä½“å‘½åä¸€è‡´æ€§(åŒä¸€å®ä½“æ˜¯å¦ä½¿ç”¨ç›¸åŒåç§°)
- å…³ç³»ä½¿ç”¨ä¸€è‡´æ€§(Giniç³»æ•°)

### 4. **å¤šæ ·æ€§è¯„åˆ†** (æƒé‡15%)
- å…³ç³»ç±»å‹å¤šæ ·æ€§
- å®ä½“ç±»å‹å¤šæ ·æ€§
- ä¿¡æ¯ç†µåˆ†æ

### 5. **Evidenceè´¨é‡** (æƒé‡10%)
- evidence_spansè¦†ç›–ç‡
- è¯æ®é•¿åº¦åˆç†æ€§
- spansæ•°é‡åˆç†æ€§

### 6. **æˆæœ¬æ•ˆç›Š** (æƒé‡10%)
- Tokenä½¿ç”¨æ•ˆç‡
- ä¸‰å…ƒç»„/Tokenæ¯”ç‡
- æ¯æ–‡æ¡£Tokenæ¶ˆè€—

### 7. **é€Ÿåº¦è¯„åˆ†**
- æ€»å¤„ç†æ—¶é—´
- å¹³å‡æ¯æ–‡æ¡£å¤„ç†æ—¶é—´

### 8. **ç»¼åˆå¾—åˆ†** (0-100åˆ†)
- å„ç»´åº¦åŠ æƒå¹³å‡

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ­¥éª¤1: å®‰è£…ä¾èµ–

```bash
cd LLMRelationExtracter
pip install -r requirements_evaluation.txt
```

### æ­¥éª¤2: å‡†å¤‡æ¨¡å‹è¾“å‡º

ç¡®ä¿ä½ å·²ç»ç”¨ä¸åŒæ¨¡å‹è¿è¡Œäº†å…³ç³»æŠ½å–:

```bash
# ä½¿ç”¨deepseek-v3
python main.py --input data/input/extracted_content.json

# ä½¿ç”¨gemini-2.5-flash (ä¿®æ”¹config.yaml)
python main.py --input data/input/extracted_content.json

# ä½¿ç”¨gpt-5 (ä¿®æ”¹config.yaml)
python main.py --input data/input/extracted_content.json
```

æ¯æ¬¡è¿è¡Œä¼šç”Ÿæˆ `knowledge_graph_<æ¨¡å‹å>.json` æ–‡ä»¶ã€‚

### æ­¥éª¤3: é…ç½®è¯„ä¼°è„šæœ¬

ç¼–è¾‘ `run_evaluation.py`,ç¡®ä¿æ–‡ä»¶è·¯å¾„æ­£ç¡®:

```python
models_to_evaluate = {
    "deepseek-v3": "data/output/knowledge_graph_deepseek.json",
    "gemini-2.5-flash": "data/output/knowledge_graph_gemini-2.5-flash.json",
    "gpt-5": "data/output/knowledge_graph_gpt-5.json",
}
```

### æ­¥éª¤4: è¿è¡Œè¯„ä¼°

```bash
python run_evaluation.py
```

---

## ğŸ“Š è¾“å‡ºæŠ¥å‘Š

è¯„ä¼°å®Œæˆå,ä¼šåœ¨ `evaluation_results/` ç›®å½•ç”Ÿæˆ:

### 1. **ExcelæŠ¥å‘Š** (`model_comparison_*.xlsx`)
åŒ…å«5ä¸ªsheet:
- ç»¼åˆå¯¹æ¯”
- è´¨é‡æŒ‡æ ‡
- Schemaç¬¦åˆåº¦
- æ€§èƒ½ä¸æˆæœ¬
- è¯¦ç»†æŒ‡æ ‡

### 2. **MarkdownæŠ¥å‘Š** (`model_comparison_*.md`)
åŒ…å«:
- ç»¼åˆæ’åè¡¨
- å„ç»´åº¦æœ€ä½³æ¨¡å‹
- è¯¦ç»†åˆ†æ

### 3. **å¯è§†åŒ–å›¾è¡¨** (PNGæ ¼å¼)
- ç»¼åˆé›·è¾¾å›¾
- å„ç»´åº¦å¯¹æ¯”æŸ±çŠ¶å›¾
- æ€§èƒ½-è´¨é‡æ•£ç‚¹å›¾
- ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾

### 4. **JSONæ•°æ®** (`model_comparison_*.json`)
å®Œæ•´çš„è¯„ä¼°ç»“æœæ•°æ®

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€ä½¿ç”¨

```python
from model_evaluator import ModelEvaluator

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = ModelEvaluator(output_dir="evaluation_results")

# åŠ è½½æ¨¡å‹è¾“å‡º
evaluator.load_model_output("deepseek-v3", "data/output/knowledge_graph_deepseek.json")
evaluator.load_model_output("gemini-2.5", "data/output/knowledge_graph_gemini.json")
evaluator.load_model_output("gpt-5", "data/output/knowledge_graph_gpt5.json")

# æ‰§è¡Œè¯„ä¼°
results = evaluator.evaluate_all_models()

# ç”ŸæˆæŠ¥å‘Š
evaluator.generate_comparison_report()
```

### æŸ¥çœ‹è¯„ä¼°ç»“æœ

```python
# è·å–æŸä¸ªæ¨¡å‹çš„è¯„ä¼°ç»“æœ
result = results["deepseek-v3"]

print(f"ç»¼åˆå¾—åˆ†: {result['overall_score']}")
print(f"è´¨é‡è¯„åˆ†: {result['quality_scores']['confidence_score']}")
print(f"Schemaç¬¦åˆåº¦: {result['schema_compliance']['schema_score']}")
print(f"ä¸‰å…ƒç»„æ€»æ•°: {result['basic_stats']['total_triplets']}")
```

---

## ğŸ” è¯„ä¼°æŒ‡æ ‡è¯¦è§£

### è´¨é‡è¯„åˆ† (Quality Score)

**è®¡ç®—å…¬å¼**:
```
è´¨é‡è¯„åˆ† = å¹³å‡ç½®ä¿¡åº¦Ã—50 + é«˜ç½®ä¿¡åº¦æ¯”ä¾‹Ã—30 - ä½ç½®ä¿¡åº¦æ¯”ä¾‹Ã—20 - æ ‡å‡†å·®æƒ©ç½šÃ—10
```

**è§£è¯»**:
- 90-100åˆ†: æé«˜è´¨é‡,ç½®ä¿¡åº¦é«˜ä¸”ç¨³å®š
- 70-90åˆ†: è‰¯å¥½è´¨é‡,å¤§éƒ¨åˆ†ä¸‰å…ƒç»„å¯ä¿¡
- 50-70åˆ†: ä¸­ç­‰è´¨é‡,éœ€äººå·¥å®¡æ ¸
- <50åˆ†: è´¨é‡è¾ƒä½,å»ºè®®è°ƒä¼˜

### Schemaç¬¦åˆåº¦ (Schema Compliance)

**è®¡ç®—å…¬å¼**:
```
Schemaè¯„åˆ† = é…ç½®å†…æ¯”ä¾‹Ã—60 + å…³ç³»ç¬¦åˆåº¦Ã—20 + å®ä½“ç¬¦åˆåº¦Ã—20
```

**è§£è¯»**:
- 90-100åˆ†: å®Œå…¨ç¬¦åˆé¢„å®šä¹‰Schema
- 70-90åˆ†: å¤§éƒ¨åˆ†ç¬¦åˆ,å°‘é‡æ–°ç±»å‹
- 50-70åˆ†: é€‚åº¦å¹³è¡¡,å‘ç°è¾ƒå¤šæ–°ç±»å‹
- <50åˆ†: åç¦»Schemaè¾ƒå¤š

**æ³¨æ„**: é«˜Schemaç¬¦åˆåº¦ä¸ä¸€å®šæœ€å¥½,å¯èƒ½é—æ¼æœ‰ä»·å€¼çš„æ–°å…³ç³»ã€‚

### ä¸€è‡´æ€§è¯„åˆ† (Consistency Score)

**å®ä½“å‘½åä¸€è‡´æ€§**:
- è¯„ä¼°åŒä¸€å®ä½“æ˜¯å¦ä½¿ç”¨ç›¸åŒåç§°
- ä¾‹: "æ ¼åŠ›ç”µå™¨"ã€"æ ¼åŠ›"ã€"GREE" æ˜¯å¦ç»Ÿä¸€

**å…³ç³»ä½¿ç”¨ä¸€è‡´æ€§**:
- ä½¿ç”¨Giniç³»æ•°è¡¡é‡å…³ç³»åˆ†å¸ƒé›†ä¸­åº¦
- é€‚åº¦é›†ä¸­(ä¸è¦è¿‡äºåˆ†æ•£)æœ€ä½³

### å¤šæ ·æ€§è¯„åˆ† (Diversity Score)

**è¯„ä¼°æŒ‡æ ‡**:
- å…³ç³»ç±»å‹æ•°é‡
- å®ä½“ç±»å‹æ•°é‡
- ä¿¡æ¯ç†µ(åˆ†å¸ƒå‡åŒ€åº¦)

**è§£è¯»**:
- é«˜å¤šæ ·æ€§: è¦†ç›–å¹¿æ³›çš„ä¿¡æ¯
- ä½å¤šæ ·æ€§: å¯èƒ½é—æ¼é‡è¦ä¿¡æ¯

### Evidenceè´¨é‡ (Evidence Quality)

**è¯„ä¼°æŒ‡æ ‡**:
- `evidence_spans`è¦†ç›–ç‡(éç©ºæ¯”ä¾‹)
- å¹³å‡evidenceé•¿åº¦(50å­—ç¬¦ä¸ºæœ€ä½³)
- å¹³å‡spansæ•°é‡(2ä¸ªä¸ºæœ€ä½³)

### æˆæœ¬æ•ˆç›Š (Cost Efficiency)

**å…³é”®æŒ‡æ ‡**:
- **ä¸‰å…ƒç»„/1K Tokens**: è¶Šé«˜è¶Šå¥½
- **Token/ä¸‰å…ƒç»„**: è¶Šä½è¶Šå¥½

**ç¤ºä¾‹**:
- æ¨¡å‹A: 1000 tokensæå–10ä¸ªä¸‰å…ƒç»„ â†’ æ•ˆç‡10/1K
- æ¨¡å‹B: 2000 tokensæå–15ä¸ªä¸‰å…ƒç»„ â†’ æ•ˆç‡7.5/1K
- æ¨¡å‹Aæ›´é«˜æ•ˆ

---

## ğŸ“ˆ å¦‚ä½•è§£è¯»è¯„ä¼°æŠ¥å‘Š

### ç»¼åˆæ’å

æŸ¥çœ‹ `ç»¼åˆå¯¹æ¯”` sheetæˆ–MarkdownæŠ¥å‘Šçš„æ’åè¡¨:
1. **ç»¼åˆå¾—åˆ†æœ€é«˜** = æ•´ä½“æ€§èƒ½æœ€ä½³
2. **ä¸‰å…ƒç»„æ•°é‡** = æå–èƒ½åŠ›
3. **å¤„ç†æ—¶é—´** = æ•ˆç‡

### å„ç»´åº¦å† å†›

ä¸åŒåœºæ™¯é€‰æ‹©ä¸åŒæ¨¡å‹:
- **è¿½æ±‚è´¨é‡**: é€‰è´¨é‡è¯„åˆ†æœ€é«˜çš„
- **ä¸¥æ ¼Schema**: é€‰Schemaç¬¦åˆåº¦æœ€é«˜çš„
- **æ¢ç´¢å‘ç°**: é€‰å¤šæ ·æ€§è¯„åˆ†æœ€é«˜çš„
- **æˆæœ¬æ•æ„Ÿ**: é€‰æˆæœ¬æ•ˆç›Šæœ€é«˜çš„

### é›·è¾¾å›¾åˆ†æ

- **å‡è¡¡å‹**: å„ç»´åº¦å¾—åˆ†æ¥è¿‘,æ— æ˜æ˜¾çŸ­æ¿
- **ä¸“ç²¾å‹**: æŸäº›ç»´åº¦çªå‡º,å…¶ä»–ä¸€èˆ¬
- **æƒè¡¡å‹**: è´¨é‡é«˜ä½†é€Ÿåº¦æ…¢,æˆ–åä¹‹

### æ•£ç‚¹å›¾åˆ†æ

**æ€§èƒ½-è´¨é‡æ•£ç‚¹å›¾**:
- å³ä¸Šè§’: åˆå¿«åˆå¥½(ç†æƒ³)
- å³ä¸‹è§’: å¿«ä½†è´¨é‡ä¸€èˆ¬
- å·¦ä¸Šè§’: æ…¢ä½†è´¨é‡å¥½
- å·¦ä¸‹è§’: åˆæ…¢åˆå·®(éœ€ä¼˜åŒ–)

---

## ğŸ¨ è‡ªå®šä¹‰è¯„ä¼°

### ä¿®æ”¹æƒé‡

ç¼–è¾‘ `model_evaluator.py` ä¸­çš„ `_compute_overall_score` å‡½æ•°:

```python
weights = {
    'quality_scores': 0.30,        # è´¨é‡æƒé‡
    'schema_compliance': 0.20,     # Schemaæƒé‡
    'consistency_scores': 0.15,    # ä¸€è‡´æ€§æƒé‡
    'diversity_scores': 0.15,      # å¤šæ ·æ€§æƒé‡
    'evidence_quality': 0.10,      # Evidenceæƒé‡
    'cost_efficiency': 0.10,       # æˆæœ¬æƒé‡
}
```

### æ·»åŠ æ–°ç»´åº¦

åœ¨ `ModelEvaluator` ç±»ä¸­æ·»åŠ æ–°çš„è¯„ä¼°å‡½æ•°:

```python
def _compute_your_metric(self, triplets: List[Dict]) -> Dict[str, Any]:
    """ä½ çš„è‡ªå®šä¹‰æŒ‡æ ‡"""
    # å®ç°ä½ çš„è¯„ä¼°é€»è¾‘
    return {
        'your_metric': value,
        'your_score': score
    }
```

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: æŸä¸ªæ¨¡å‹çš„æ–‡ä»¶åŠ è½½å¤±è´¥?

**åŸå› **: æ–‡ä»¶è·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®

**è§£å†³**:
1. æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨: `ls data/output/knowledge_graph_*.json`
2. éªŒè¯JSONæ ¼å¼: `python -m json.tool < æ–‡ä»¶è·¯å¾„`
3. ç¡®ä¿æ–‡ä»¶æ˜¯ç”± `main.py` ç”Ÿæˆçš„å®Œæ•´æ ¼å¼

### Q2: è¯„ä¼°ç»“æœä¸º0æˆ–å¼‚å¸¸?

**åŸå› **: è¾“å…¥æ•°æ®ç¼ºå°‘å¿…è¦å­—æ®µ

**è§£å†³**:
1. ç¡®ä¿çŸ¥è¯†å›¾è°±åŒ…å« `triplets.all` å­—æ®µ
2. ç¡®ä¿åŒ…å« `metadata.statistics` å­—æ®µ
3. æ£€æŸ¥ä¸‰å…ƒç»„æ˜¯å¦åŒ…å« `confidence`, `evidence_spans` ç­‰å­—æ®µ

### Q3: å›¾è¡¨ä¸æ˜¾ç¤ºä¸­æ–‡?

**åŸå› **: matplotlibç¼ºå°‘ä¸­æ–‡å­—ä½“

**è§£å†³**:
1. Windows: è„šæœ¬å·²é…ç½® `SimHei` æˆ– `Microsoft YaHei`
2. Mac: ä½¿ç”¨ `Arial Unicode MS`
3. Linux: å®‰è£…å­—ä½“åŒ…: `sudo apt-get install fonts-noto-cjk`

### Q4: æƒ³è¦æ›´è¯¦ç»†çš„å¯¹æ¯”?

**è§£å†³**:
1. æŸ¥çœ‹Excelçš„ `è¯¦ç»†æŒ‡æ ‡` sheet
2. æŸ¥çœ‹JSONæ–‡ä»¶è·å–åŸå§‹æ•°æ®
3. ä½¿ç”¨Pandasè¯»å–è¿›è¡Œè‡ªå®šä¹‰åˆ†æ

---

## ğŸ“š è¿›é˜¶ç”¨æ³•

### æ‰¹é‡è¯„ä¼°å¤šä¸ªé…ç½®

```python
# è¯„ä¼°ä¸åŒtemperatureçš„æ•ˆæœ
configs = [
    ("deepseek-t0.0", "output/kg_deepseek_t0.0.json"),
    ("deepseek-t0.1", "output/kg_deepseek_t0.1.json"),
    ("deepseek-t0.3", "output/kg_deepseek_t0.3.json"),
]

evaluator = ModelEvaluator()
for name, path in configs:
    evaluator.load_model_output(name, path)

evaluator.evaluate_all_models()
evaluator.generate_comparison_report()
```

### å¯¼å‡ºä¸ºLaTeXè¡¨æ ¼

```python
import pandas as pd

# è¯»å–Excel
df = pd.read_excel("evaluation_results/model_comparison_*.xlsx", sheet_name="ç»¼åˆå¯¹æ¯”")

# å¯¼å‡ºLaTeX
latex = df.to_latex(index=False)
print(latex)
```

### ç”Ÿæˆè®ºæ–‡çº§å›¾è¡¨

```python
# ä¿®æ”¹ model_evaluator.py ä¸­çš„ç»˜å›¾å‚æ•°
plt.rcParams['figure.dpi'] = 600  # æ›´é«˜åˆ†è¾¨ç‡
plt.rcParams['font.size'] = 14    # æ›´å¤§å­—ä½“
sns.set_context("paper")          # è®ºæ–‡é£æ ¼
```

---

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤Issueå’ŒPull Requestæ”¹è¿›è¯„ä¼°ç³»ç»Ÿï¼

å¯ä»¥æ”¹è¿›çš„æ–¹å‘:
- æ·»åŠ æ–°çš„è¯„ä¼°ç»´åº¦
- æ”¹è¿›è¯„åˆ†ç®—æ³•
- å¢åŠ å¯è§†åŒ–å›¾è¡¨ç±»å‹
- æ”¯æŒæ›´å¤šè¾“å‡ºæ ¼å¼

---

## ğŸ“„ è®¸å¯è¯

MIT License

---

**ç‰ˆæœ¬**: v1.0
**æœ€åæ›´æ–°**: 2025-10-10
**ä½œè€…**: AgentCompetitionAnalysisPlatform Team
