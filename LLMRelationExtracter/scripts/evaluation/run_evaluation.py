#!/usr/bin/env python3
"""
è¿è¡Œæ¨¡å‹è¯„ä¼°çš„ä¾¿æ·è„šæœ¬

ä½¿ç”¨æ–¹æ³•:
  cd LLMRelationExtracter
  python scripts/evaluation/run_evaluation.py
"""

import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '../..'))
sys.path.insert(0, project_root)

from scripts.evaluation.model_evaluator import ModelEvaluator

def main():
    """ä¸»å‡½æ•°"""
    print("="*80)
    print("ğŸš€ å¯åŠ¨æ¨¡å‹æ€§èƒ½è‡ªåŠ¨åŒ–è¯„ä¼°")
    print("="*80)

    # åˆ‡æ¢åˆ°é¡¹ç›®æ ¹ç›®å½•
    os.chdir(project_root)
    print(f"å·¥ä½œç›®å½•: {os.getcwd()}\n")

    # åˆ›å»ºè¯„ä¼°å™¨ï¼Œè¾“å‡ºåˆ° data/output/evaluation_results
    evaluator = ModelEvaluator(output_dir="data/output/evaluation_results")

    # é…ç½®è¦è¯„ä¼°çš„æ¨¡å‹
    # æ ¹æ®ä½ çš„å®é™…è¾“å‡ºæ–‡ä»¶é…ç½®
    models_to_evaluate = {
        "deepseek": "data/output/knowledge_graph_deepseek.json",
        "gemini-2.5-flash": "data/output/knowledge_graph_gemini-2.5-flash.json",
        "gpt-5": "data/output/knowledge_graph_gpt-5.json",
    }

    print(f"\nğŸ“ å‡†å¤‡è¯„ä¼°ä»¥ä¸‹æ¨¡å‹:")
    for model_name in models_to_evaluate.keys():
        print(f"  - {model_name}")

    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨å¹¶åŠ è½½
    print(f"\nğŸ“¥ åŠ è½½æ¨¡å‹è¾“å‡ºæ–‡ä»¶...")
    loaded_models = []
    for model_name, file_path in models_to_evaluate.items():
        if os.path.exists(file_path):
            try:
                evaluator.load_model_output(model_name, file_path)
                loaded_models.append(model_name)
            except Exception as e:
                print(f"  âŒ åŠ è½½ {model_name} å¤±è´¥: {e}")
        else:
            print(f"  âš ï¸  æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")

    if not loaded_models:
        print("\nâŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æ¨¡å‹è¾“å‡ºæ–‡ä»¶")
        print("è¯·ç¡®ä¿:")
        print("  1. å·²ç»è¿è¡Œè¿‡ main.py ç”ŸæˆçŸ¥è¯†å›¾è°±")
        print("  2. æ–‡ä»¶è·¯å¾„é…ç½®æ­£ç¡®")
        print(f"  3. åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ: cd {project_root}")
        return 1

    print(f"\nâœ… æˆåŠŸåŠ è½½ {len(loaded_models)} ä¸ªæ¨¡å‹çš„è¾“å‡º:")
    for model in loaded_models:
        print(f"  âœ“ {model}")

    # æ‰§è¡Œè¯„ä¼°
    print(f"\n{'='*80}")
    print("ğŸ“Š å¼€å§‹è‡ªåŠ¨åŒ–è¯„ä¼°...")
    print(f"{'='*80}")

    results = evaluator.evaluate_all_models()

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    print(f"\n{'='*80}")
    print("ğŸ“ ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š...")
    print(f"{'='*80}")

    evaluator.generate_comparison_report()

    # è¾“å‡ºå¿«é€Ÿæ‘˜è¦
    print(f"\n{'='*80}")
    print("ğŸ† è¯„ä¼°ç»“æœæ‘˜è¦")
    print(f"{'='*80}\n")

    # æŒ‰ç»¼åˆå¾—åˆ†æ’åº
    ranking = sorted(results.items(), key=lambda x: x[1]['overall_score'], reverse=True)

    print("ç»¼åˆæ’å:")
    for rank, (model, result) in enumerate(ranking, 1):
        medal = "ğŸ¥‡" if rank == 1 else "ğŸ¥ˆ" if rank == 2 else "ğŸ¥‰" if rank == 3 else f"{rank}."
        print(f"  {medal} {model:20s} - ç»¼åˆå¾—åˆ†: {result['overall_score']}/100")

    print(f"\nå„ç»´åº¦æœ€ä½³:")

    dimensions = {
        'è´¨é‡': ('quality_scores', 'confidence_score'),
        'Schemaç¬¦åˆåº¦': ('schema_compliance', 'schema_score'),
        'ä¸€è‡´æ€§': ('consistency_scores', 'consistency_score'),
        'å¤šæ ·æ€§': ('diversity_scores', 'diversity_score'),
        'Evidenceè´¨é‡': ('evidence_quality', 'evidence_score'),
        'æˆæœ¬æ•ˆç›Š': ('cost_efficiency', 'cost_efficiency_score'),
        'é€Ÿåº¦': ('performance_metrics', 'speed_score'),
    }

    for dim_name, (cat, key) in dimensions.items():
        best_model = max(results.items(), key=lambda x: x[1][cat][key])
        score = best_model[1][cat][key]
        print(f"  ğŸŒŸ {dim_name:12s}: {best_model[0]:20s} ({score:.1f}/100)")

    print(f"\n{'='*80}")
    print("âœ… è¯„ä¼°å®Œæˆï¼")
    print(f"{'='*80}")
    print(f"\nğŸ“‚ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: data/output/evaluation_results/")
    print(f"  - ExcelæŠ¥å‘Š (model_comparison_*.xlsx)")
    print(f"  - MarkdownæŠ¥å‘Š (model_comparison_*.md)")
    print(f"  - å¯è§†åŒ–å›¾è¡¨ (*.png)")
    print(f"  - JSONæ•°æ® (model_comparison_*.json)")

    return 0


if __name__ == "__main__":
    sys.exit(main())
